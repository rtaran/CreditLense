{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher, HtmlDiff\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# LLM Report Comparison Tool\n",
    "#\n",
    "# This notebook helps you compare financial reports generated by different LLM providers\n",
    "# (OpenAI vs Google) to verify differences and analyze the outputs.\n",
    "\n",
    "# Database connection function\n",
    "def get_db_connection():\n",
    "    \"\"\"Connect to the SQLite database specified in the environment variables\"\"\"\n",
    "    from app.database import DATABASE_URL\n",
    "    db_path = DATABASE_URL.replace(\"sqlite:///\", \"\")\n",
    "    return sqlite3.connect(db_path)\n",
    "\n",
    "\n",
    "def get_memos_by_provider():\n",
    "    \"\"\"Fetch memos from the database grouped by LLM provider\"\"\"\n",
    "    conn = get_db_connection()\n",
    "\n",
    "    # Query to get all financial memos with their LLM provider\n",
    "    query = \"\"\"\n",
    "            SELECT id,\n",
    "                   company_name,\n",
    "                   report_date,\n",
    "                   summary,\n",
    "                   analysis,\n",
    "                   recommendations,\n",
    "                   file_path,\n",
    "                   llm_provider,\n",
    "                   created_at\n",
    "            FROM financial_memos\n",
    "            ORDER BY created_at DESC \\\n",
    "            \"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Group by provider\n",
    "    return {\n",
    "        'openai': df[df['llm_provider'] == 'openai'],\n",
    "        'google': df[df['llm_provider'] == 'google'],\n",
    "        'unknown': df[df['llm_provider'].isnull()]\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_reports_for_same_document(document_id=None):\n",
    "    \"\"\"\n",
    "    Compare reports generated by different LLMs for the same input document\n",
    "\n",
    "    If document_id is None, it will find documents that have reports from multiple providers\n",
    "    \"\"\"\n",
    "    conn = get_db_connection()\n",
    "\n",
    "    if document_id is None:\n",
    "        # Find documents that have reports from multiple providers\n",
    "        query = \"\"\"\n",
    "                SELECT company_data_id, COUNT(DISTINCT llm_provider) as provider_count\n",
    "                FROM financial_memos\n",
    "                WHERE llm_provider IS NOT NULL\n",
    "                GROUP BY company_data_id\n",
    "                HAVING provider_count > 1 \\\n",
    "                \"\"\"\n",
    "        multi_provider_docs = pd.read_sql_query(query, conn)\n",
    "\n",
    "        if len(multi_provider_docs) == 0:\n",
    "            conn.close()\n",
    "            return \"No documents found with reports from multiple LLM providers\"\n",
    "\n",
    "        # Use the first document that has multiple providers\n",
    "        document_id = multi_provider_docs.iloc[0]['company_data_id']\n",
    "\n",
    "    # Get reports for this document\n",
    "    query = f\"\"\"\n",
    "    SELECT id, company_name, summary, analysis, recommendations, llm_provider\n",
    "    FROM financial_memos\n",
    "    WHERE company_data_id = {document_id}\n",
    "    \"\"\"\n",
    "\n",
    "    reports = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    if len(reports) < 2:\n",
    "        return f\"Only found {len(reports)} report(s) for document ID {document_id}\"\n",
    "\n",
    "    return reports\n",
    "\n",
    "\n",
    "def detect_llm_patterns(text):\n",
    "    \"\"\"\n",
    "    Analyze text to detect patterns that might indicate which LLM generated it\n",
    "    Returns a dictionary of pattern counts and confidence score\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'openai': [\n",
    "            r'\\bI think\\b',  # GPT models often use \"I think\"\n",
    "            r'\\bAs an AI\\b',  # GPT self-references\n",
    "            r'\\bHowever,\\b',  # Tendency to use contrastive language\n",
    "            r'\\bIt\\'s important to note\\b',  # Common GPT phrase\n",
    "        ],\n",
    "        'google': [\n",
    "            r'\\bBased on the information\\b',  # Common in Google models\n",
    "            r'\\bThe document indicates\\b',  # More formal, document-focused language\n",
    "            r'\\bAccording to\\b',  # More attribution-focused\n",
    "            r'\\bIt appears that\\b',  # More tentative language\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for provider, pattern_list in patterns.items():\n",
    "        matches = 0\n",
    "        for pattern in pattern_list:\n",
    "            matches += len(re.findall(pattern, text, re.IGNORECASE))\n",
    "        results[provider] = matches\n",
    "\n",
    "    # Calculate confidence score\n",
    "    total_matches = sum(results.values())\n",
    "    confidence = {}\n",
    "\n",
    "    if total_matches > 0:\n",
    "        for provider, matches in results.items():\n",
    "            confidence[provider] = matches / total_matches\n",
    "\n",
    "    return {\n",
    "        'pattern_matches': results,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "\n",
    "def display_report_comparison(reports_df):\n",
    "    \"\"\"Display a side-by-side comparison of reports from different providers\"\"\"\n",
    "    if isinstance(reports_df, str):\n",
    "        return HTML(f\"<div>{reports_df}</div>\")\n",
    "\n",
    "    providers = reports_df['llm_provider'].unique()\n",
    "\n",
    "    if len(providers) < 2:\n",
    "        return HTML(f\"<div>Need at least 2 providers to compare. Found: {', '.join(providers)}</div>\")\n",
    "\n",
    "    # Create a comparison for each section\n",
    "    sections = ['summary', 'analysis', 'recommendations']\n",
    "\n",
    "    html_output = f\"<h2>Report Comparison for {reports_df.iloc[0]['company_name']}</h2>\"\n",
    "\n",
    "    for section in sections:\n",
    "        html_output += f\"<h3>{section.title()}</h3>\"\n",
    "        html_output += \"<table style='width:100%; border-collapse: collapse;'>\"\n",
    "\n",
    "        # Header row\n",
    "        html_output += \"<tr>\"\n",
    "        for provider in providers:\n",
    "            html_output += f\"<th style='border:1px solid black; padding:8px; background-color:#f2f2f2;'>{provider}</th>\"\n",
    "        html_output += \"</tr>\"\n",
    "\n",
    "        # Content row\n",
    "        html_output += \"<tr>\"\n",
    "        for provider in providers:\n",
    "            provider_report = reports_df[reports_df['llm_provider'] == provider].iloc[0]\n",
    "            content = provider_report[section].replace('\\n', '<br>')\n",
    "\n",
    "            # Run pattern detection\n",
    "            patterns = detect_llm_patterns(provider_report[section])\n",
    "            confidence = patterns['confidence']\n",
    "            confidence_html = \"\"\n",
    "\n",
    "            if provider in confidence and confidence[provider] > 0:\n",
    "                conf_pct = confidence[provider] * 100\n",
    "                color = \"green\" if conf_pct > 70 else \"orange\"\n",
    "                confidence_html = f\"<div style='color:{color};'><small>Confidence: {conf_pct:.1f}%</small></div>\"\n",
    "\n",
    "            html_output += f\"<td style='border:1px solid black; padding:8px; vertical-align:top;'>{confidence_html}{content}</td>\"\n",
    "        html_output += \"</tr>\"\n",
    "\n",
    "        html_output += \"</table>\"\n",
    "\n",
    "        # Add a diff view\n",
    "        if len(providers) == 2:\n",
    "            html_output += \"<h4>Differences</h4>\"\n",
    "\n",
    "            text1 = reports_df[reports_df['llm_provider'] == providers[0]].iloc[0][section]\n",
    "            text2 = reports_df[reports_df['llm_provider'] == providers[1]].iloc[0][section]\n",
    "\n",
    "            differ = HtmlDiff(tabsize=4)\n",
    "            diff_html = differ.make_table(text1.splitlines(), text2.splitlines(),\n",
    "                                          providers[0], providers[1],\n",
    "                                          context=True, numlines=3)\n",
    "\n",
    "            html_output += f\"<div style='font-size:0.8em; overflow-x:scroll;'>{diff_html}</div>\"\n",
    "\n",
    "    # Add similarity metrics\n",
    "    html_output += \"<h3>Similarity Analysis</h3>\"\n",
    "\n",
    "    similarity_data = {}\n",
    "    for section in sections:\n",
    "        text1 = reports_df[reports_df['llm_provider'] == providers[0]].iloc[0][section]\n",
    "        text2 = reports_df[reports_df['llm_provider'] == providers[1]].iloc[0][section]\n",
    "\n",
    "        similarity = SequenceMatcher(None, text1, text2).ratio()\n",
    "        similarity_data[section] = similarity\n",
    "\n",
    "    similarity_df = pd.DataFrame([similarity_data])\n",
    "\n",
    "    # Convert dataframe to HTML table\n",
    "    similarity_html = similarity_df.to_html(index=False)\n",
    "    html_output += similarity_html\n",
    "\n",
    "    # Add a bar chart\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(similarity_data.keys(), similarity_data.values())\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title('Text Similarity Between Providers')\n",
    "    plt.ylabel('Similarity (0-1)')\n",
    "    plt.savefig('similarity_chart.png')\n",
    "    plt.close()\n",
    "\n",
    "    html_output += \"<img src='similarity_chart.png' style='max-width:600px;'>\"\n",
    "\n",
    "    return HTML(html_output)\n",
    "\n",
    "\n",
    "# Example usage - run this cell to compare reports\n",
    "reports = compare_reports_for_same_document()\n",
    "display_report_comparison(reports)\n",
    "\n",
    "\n",
    "# Verification of Different LLMs\n",
    "#\n",
    "# The comparison above should reveal whether the reports are truly generated by different\n",
    "# LLMs. Look for:\n",
    "#\n",
    "# 1. **Different writing styles** - Each LLM has distinct patterns\n",
    "# 2. **Different emphasis** - What facts each model focuses on\n",
    "# 3. **Confidence scores** - Based on language pattern analysis\n",
    "#\n",
    "# If the reports are nearly identical, they may be coming from the same LLM despite\n",
    "# different provider settings in your configuration.\n",
    "\n",
    "def check_llm_configuration():\n",
    "    \"\"\"Check the current LLM configuration in the .env file\"\"\"\n",
    "    provider = os.getenv(\"LLM_PROVIDER\", \"\").lower()\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    google_key = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
    "\n",
    "    print(f\"Current LLM provider: {provider}\")\n",
    "    print(f\"OpenAI API key configured: {'Yes' if openai_key else 'No'}\")\n",
    "    print(f\"Google API key configured: {'Yes' if google_key else 'No'}\")\n",
    "\n",
    "    # Check for potential issues\n",
    "    issues = []\n",
    "    if not provider:\n",
    "        issues.append(\"LLM_PROVIDER is not set\")\n",
    "    if provider == \"openai\" and not openai_key:\n",
    "        issues.append(\"Using OpenAI provider but API key is missing\")\n",
    "    if provider == \"google\" and not google_key:\n",
    "        issues.append(\"Using Google provider but API key is missing\")\n",
    "\n",
    "    if issues:\n",
    "        print(\"\\nPotential issues:\")\n",
    "        for issue in issues:\n",
    "            print(f\"- {issue}\")\n",
    "    else:\n",
    "        print(\"\\nConfiguration looks valid.\")\n",
    "\n",
    "\n",
    "check_llm_configuration()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
